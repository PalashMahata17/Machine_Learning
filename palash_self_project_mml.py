# -*- coding: utf-8 -*-
"""Palash_Self_Project_MML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vn_Cm02YrsjTk2lhMjmAmtuEK4Q8B9lp

# TOPIC:- FOOD WASTAGE RE-DISTRIBUTION NETWORK.

Idea:- It's a machine learning model that can predict how much food will be wasted at an event, just by knowing things like the event type, the season, and how many guests are coming. It's designed to help caterers ,restrudent order the right amount of food, which saves money and is also better for the environment by reducing the harmful greenhouse gases like methane which happens by the decomposition of the food waste in a region.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df=pd.read_csv("/content/food_wastage_data.csv")   # reading the file

"""## Step 1: Understanding The Data
•	The type of event (like a Wedding or Birthday)

•	The number of guests

•	The type of food served (Meat, Fruits, etc.)

•	The season

•	The actual amount of food that was wasted.

"""

df.sample(5)   #coloum view

df.shape  #no of rows and columns

df.isnull().sum()  #how many null value is present

df.info()  # name of the columns.

df.describe()  #no of numerical coloum.

Avg= df.groupby("Event Type")
mean_waste_by_event = Avg["Wastage Food Amount"].mean()
print(mean_waste_by_event)

Avg_M= df.groupby("Event Type")
mean_waste_by_event = Avg_M["Wastage Food Amount"].median()
print(mean_waste_by_event)

"""## Mean wastage for each event type and each Food Type—we can do a pivot"""

pivot = df.groupby(["Event Type","Type of Food"])["Wastage Food Amount"].mean().unstack()
pivot

"""## STEP 2:- Exploring the data to find patterns. I made a bunch of charts to answer some simple questions:
•	Which events are the most wasteful?

•	Is there a link between the number of guests and the amount of waste?

•	What about the type of food?
"""

# Univariate Analysis
# Distribution of Wastage Food Amount

sns.histplot(df["Wastage Food Amount"], bins=20, edgecolor='black',kde=True)
plt.title("Distribution of Wasted Food (kg)")
plt.show()

sns.histplot(df["Quantity of Food"], kde=True)
plt.title("Distribution of Quantity of Food")
plt.show()

# Counts of Event Types & Storage Conditions
event_type_counts = df["Event Type"].value_counts(ascending=False)
sns.countplot(data=df,y="Event Type",order=event_type_counts.index,edgecolor='black')
plt.show()

sns.countplot(data=df,x="Storage Conditions",edgecolor='black')
plt.show()

avg_wastage_by_event = df.groupby('Event Type')['Wastage Food Amount'].mean().sort_values(ascending=False)

sns.barplot(data=df, x='Event Type', y='Wastage Food Amount', order=avg_wastage_by_event.index)
plt.title('Average Food Wastage by Event Type')
plt.ylabel('Average Wastage Food Amount (kg)')
plt.xlabel('Event Type')
plt.show()

# Bivariate Relationships
# Wastage vs. Number of Guests
sns.scatterplot(data=df,x="Number of Guests",
                y="Wastage Food Amount")
plt.show()

Q1 = df["Wastage Food Amount"].quantile(0.25)
Q3 = df["Wastage Food Amount"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df_no_outliers = df[(df["Wastage Food Amount"] >= lower_bound) &
                    (df["Wastage Food Amount"] <= upper_bound)]

# Boxplot of Wastage by Seasonality
sns.boxplot(data=df,x="Seasonality",
            y="Wastage Food Amount")
plt.show()

corr = df[["Number of Guests","Quantity of Food","Wastage Food Amount"]].corr()
sns.heatmap(corr, annot=True,fmt='.2f', cmap="coolwarm")  # annote is used to write the value inside the matrix, fmt=.2f means 2 decimal places.
plt.title("Correlation Matrix")
plt.show()

# Correlation matrix between numerical variables
cols = ["Number of Guests", "Quantity of Food", "Wastage Food Amount"]
correlation_matrix = df[cols].corr()

# Print the correlation matrix
correlation_matrix

for col in ["Event Type","Type of Food","Seasonality","Storage Conditions"]:
    plt.figure(figsize=(6,4))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index)
    plt.title(f"Count of {col}")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Visualization of Preparation Method distribution
sns.countplot(data=df, x="Preparation Method", edgecolor='black')
plt.title("Distribution of Preparation Method")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Visualization of Geographical Location distribution
sns.countplot(data=df, x="Geographical Location", edgecolor='black')
plt.title("Distribution of Geographical Location")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Visualization of Pricing distribution
sns.countplot(data=df, x="Pricing", edgecolor='black')
plt.title("Distribution of Pricing")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#Model selection and making

# Features Selection
df['waste_ratio'] = df['Wastage Food Amount'] / df['Quantity of Food']
df['guests_per_food'] = df['Number of Guests'] / df['Quantity of Food']


# Target
y = df["Wastage Food Amount"]

# Baseline features
X = df[[
    "Number of Guests",
    "Quantity of Food",
    # engineered:
    "waste_ratio",
    "guests_per_food",
    # plus categorical:
    "Event Type",
    "Type of Food",
    "Storage Conditions",
    "Seasonality"
]]

"""# Building the The Model :-
1.	Splitting the Data: I split my data into a "training set" (80%) and "testing set" (20%).
2.	Preparing the Data for the Model: Computers only understand numbers, so I had to translate text-based columns like "Event Type" into a numerical format. I used a standard technique called One-Hot Encoding. I bundled this step into a "pipeline" to keep everything organized.

3.	Trying Different Models: I didn’t just build one model but I used four different types of Model (Linear Regression, Decision Tree, Random Forest, and Gradient Boosting). to see whick model works best for making a prediction.
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

num_cols = ["Number of Guests","Quantity of Food","waste_ratio","guests_per_food"]
cat_cols = ["Event Type","Type of Food","Storage Conditions","Seasonality"]

num_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

cat_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="constant", fill_value="Missing")),
    ("ohe", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", num_pipe, num_cols),
    ("cat", cat_pipe, cat_cols)
])

#  Preprocessing: numeric scaling + one-hot for categoricals

# Define the list of all feature columns
FEATURES = ["Number of Guests", "Quantity of Food",
            "waste_ratio", "guests_per_food", "Event Type",
            "Type of Food", "Storage Conditions", "Seasonality"]

numeric_feats = ["Number of Guests", "Quantity of Food",
                 "waste_ratio", "guests_per_food"]

categorical_feats = list(set(FEATURES) - set(numeric_feats))

numeric_transformer = Pipeline(steps=[("scaler", StandardScaler())])

categorical_transformer = Pipeline(steps=[
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_feats),
            ("cat", categorical_transformer, categorical_feats)
])

#  Baseline Model 1 – Linear Regression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error # Import r2_score, MAE, MSE
from sklearn.linear_model import LinearRegression # Import LinearRegression

lin_pipeline = Pipeline(steps=[("prep", preprocessor),
                               ("model", LinearRegression())])

lin_pipeline.fit(X_train, y_train)
lin_pred = lin_pipeline.predict(X_test)

def report(y_true, y_pred, model_name, n, k):
    print(f"\n{model_name}")
    print("MAE :", mean_absolute_error(y_true, y_pred))
    print("RMSE:", mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print("R²  :", r2)
    adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))
    print("adjusted_r2 : ", adjusted_r2)

# Calculate n and k for the test set
n = len(y_test)
k = X_test.shape[1]

report(y_test, lin_pred, "Linear Regression", n, k)

#  Model 2 – Random Forest with Hyperparameter Search
from sklearn.ensemble import RandomForestRegressor # Import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV # Import RandomizedSearchCV

rf = RandomForestRegressor(random_state=42)

rf_params = {
    "model__n_estimators": [100, 250, 500],
    "model__max_depth": [None, 10, 20],
    "model__min_samples_split": [2, 5, 10]
}

rf_pipeline = Pipeline(steps=[("prep", preprocessor),
                              ("model", rf)])

rf_search = RandomizedSearchCV(rf_pipeline, rf_params,
                               n_iter=10, cv=5,
                               scoring="neg_mean_absolute_error",
                               random_state=42, n_jobs=-1)

rf_search.fit(X_train, y_train)
rf_pred = rf_search.best_estimator_.predict(X_test)

# Calculate n and k for the test set if not already defined
if 'n' not in locals() or 'k' not in locals():
    n = len(y_test)
    k = X_test.shape[1]

report(y_test, rf_pred, "Random Forest (tuned)", n, k)

# 14. Model 3 – Gradient Boosting Regressor
from sklearn.ensemble import GradientBoostingRegressor # Import GradientBoostingRegressor

gbr = GradientBoostingRegressor(random_state=42)

gbr_params = {
    "model__n_estimators": [200, 400, 600],
    "model__learning_rate": [0.05, 0.1, 0.2],
    "model__max_depth": [2, 3, 4]
}

gbr_pipeline = Pipeline(steps=[("prep", preprocessor),
                               ("model", gbr)])

gbr_search = RandomizedSearchCV(gbr_pipeline, gbr_params,
                                n_iter=10, cv=5,
                                scoring="neg_mean_absolute_error",
                                random_state=42, n_jobs=-1)

gbr_search.fit(X_train, y_train)
gbr_pred = gbr_search.best_estimator_.predict(X_test)

# Calculate n and k for the test set if not already defined
if 'n' not in locals() or 'k' not in locals():
    n = len(y_test)
    k = X_test.shape[1]

report(y_test, gbr_pred, "Gradient Boosting (tuned)", n, k)

# Model 4 – Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor

dt = DecisionTreeRegressor(random_state=42)

dt_pipeline = Pipeline(steps=[("prep", preprocessor),
                               ("model", dt)])

dt_pipeline.fit(X_train, y_train)
dt_pred = dt_pipeline.predict(X_test)

# Calculate n and k for the test set if not already defined
if 'n' not in locals() or 'k' not in locals():
    n = len(y_test)
    k = X_test.shape[1]

report(y_test, dt_pred, "Decision Tree", n, k)

# Model 5 – XGBoost Regressor
from xgboost import XGBRegressor

xgb = XGBRegressor(random_state=42)

xgb_pipeline = Pipeline(steps=[("prep", preprocessor),
                               ("model", xgb)])

xgb_pipeline.fit(X_train, y_train)
xgb_pred = xgb_pipeline.predict(X_test)

# Calculate n and k for the test set if not already defined
if 'n' not in locals() or 'k' not in locals():
    n = len(y_test)
    k = X_test.shape[1]

# Use the k_features calculated from the preprocessor
report(y_test, xgb_pred, "XGBoost", n, k_features)

# 6) Define models to compare
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline

models = {
    "LinearRegression": LinearRegression(),
    "DecisionTree":     DecisionTreeRegressor(random_state=42),
    "RandomForest":     RandomForestRegressor(n_estimators=100, random_state=42),
    "GBM":              GradientBoostingRegressor(n_estimators=100, random_state=42),
    "XGBoost":          XGBRegressor(random_state=42)
}

# Fit the preprocessor on the training data to learn the feature transformations
preprocessor.fit(X_train)
k_features = preprocessor.get_feature_names_out().shape[0]

# 7)  Train, predict, and evaluate each model ---
for name, estimator in models.items():
    # Create the full pipeline
    pipe = Pipeline([
        ("prep", preprocessor),
        ("model", estimator)
    ])

    # Fit the pipeline on the training data
    pipe.fit(X_train, y_train)

    # Make predictions on the test data
    preds = pipe.predict(X_test)

    # Use the updated report function with k_features
    report(y_test, preds, name, n, k_features)

"""# Summary:-
After training each model, I used the "testing set" to see how accurate the predictions were. The main grade that i used was the metric **RMSE**(Root Mean Square Error) and found out that **XGBoost** is the best among all the methods the model explanibility can be told by using LIME/SHAP.

# **The Business Problem I'm Trying to Solve::**


Waste is expensive. For any business that deals with food, like a catering company or a hotel, leftover food is like money thrown in the bin. They have to balance ordering enough food so they don't run out, but not so much that they lose profits on waste.
So, the business question is: "Can we accurately predict how much food will be wasted at a specific event so we can optimize our food orders?"
Solving this doesn't just save money; it's also a huge win for sustainability and the environment.

"""

